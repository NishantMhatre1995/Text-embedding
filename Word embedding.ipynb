{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ad1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae6e08",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reviews.csv', nrows=10000)\n",
    "data = data[data.Score!=3] # 1,2 bad - 3 normal - 4,5 good feedback\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba258e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c970f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actualScore = data['Score']\n",
    "data['flag'] = data['Score'].apply(lambda x: 0 if x<4 else 1) # +ve or -ve review flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates entries\n",
    "data = data.sort_values('ProductId').drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "data = data[data.HelpfulnessNumerator <= data.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de830af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Score.value_counts() \n",
    "# data is biased, need to do sampling while building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb223248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4561948a",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ba476",
   "metadata": {},
   "source": [
    "NLP text preprocessing\n",
    "- Remove the html tags first.\n",
    "- Eliminate all punctuation and a select group of special characters, such as, or., or #, etc.\n",
    "- Verify that the term is composed of English letters alone and not any other characters.\n",
    "- Make sure the term is longer than two by measuring its length (as it was researched that there is no adjective in 2-letters)\n",
    "- Change the word's case to lowercase.\n",
    "- Eliminate Stopwords\n",
    "- Snowball Stemming (it was obsereved to be better than Porter Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop.remove('not')\n",
    "stop.remove('but')\n",
    "# 'not' is useful word as it changes the meaning of sentence\n",
    "snoStem = nltk.stem.SnowballStemmer('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381057d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    def removeTag(sentence): \n",
    "        clr = re.compile('<.*?>')\n",
    "        text = re.sub(clr, ' ', sentence)\n",
    "        return text\n",
    "\n",
    "    def removePunctuation(sentence): \n",
    "        text = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        text = re.sub(r'[.|,|)|(|\\|/]',r' ',text)\n",
    "        return  text\n",
    "\n",
    "    def removeUrls(text):\n",
    "        return re.sub(r'https?://\\S+|www.\\.\\S+', '', text)\n",
    "\n",
    "    def removeStopwords(text, stopwords=None):\n",
    "        if stopwords is None:\n",
    "            stopwords = set(stopwords.words('english'))\n",
    "        return ' '.join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "    def lemmaSentence(text, lemmatizer):\n",
    "        new_text = ''\n",
    "        tok_text = word_tokenize(text)\n",
    "        tags = nltk.pos_tag(tok_text)\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        tags = [tag_dict.get(tag[1][0],  wordnet.NOUN) for tag in tags]\n",
    "        for i in range(len(tok_text)):\n",
    "            new_text = new_text + ' ' + lemmatizer.lemmatize(tok_text[i], tags[i])\n",
    "        return new_text[1:]\n",
    "\n",
    "    def stemmingSentence(text):\n",
    "        return snoStem.stem(text)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Function calling\n",
    "    text = text.lower()\n",
    "    text = removeStopwords(text, stop)\n",
    "    text = removeTag(text)\n",
    "    text = removePunctuation(text)\n",
    "    text = stemmingSentence(text)\n",
    "    text = lemmaSentence(text, WordNetLemmatizer())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform preprocessing\n",
    "data['clean_text'] = data['Text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7911f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['clean_text'].values\n",
    "y = data['flag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2d219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "026591f6",
   "metadata": {},
   "source": [
    "## BoW (Bag of Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadff0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() \n",
    "final_counts = count_vect.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b67742",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_counts.shape #check number of features we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfd8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "025d1132",
   "metadata": {},
   "source": [
    "## Bi-Grams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8d9d9",
   "metadata": {},
   "source": [
    "For simplicity just look at growth of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44765a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2) ) #in scikit-learn\n",
    "final_bigram_counts = count_vect.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bigram_counts.shape # just with bi-gram feature size grow exponentially. just think what happens with3 or 4 grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9834f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c71d761b",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbeb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(min_df=5, max_features=10000, ngram_range=(1,2), lowercase=False, tokenizer=word_tokenize)\n",
    "x_tf = tfidf_vect.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9f2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73e41e73",
   "metadata": {},
   "source": [
    "## Avg. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf27ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_token = [word_tokenize(sentence) for sentence in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "model_w2v = Word2Vec(min_count=8,window=3,sample=6e-5, alpha=0.02, min_alpha=0.0005, negative=15)\n",
    "model_w2v.build_vocab(x_token) # Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ec39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.train(x_token, total_examples=model_w2v.corpus_count, epochs=20, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save and use model for future use\n",
    "# model_w2v.save(\"Aw2v.model\")\n",
    "# model_w2v = gensim.models.word2vec.Word2Vec.load(\"Aw2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dda520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.wv.most_similar('food', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29027d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean vector calculation\n",
    "def AW2V_eachWord(model_w2v, words):\n",
    "    words = [word for word in words if word in model_w2v.wv.vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model_w2v[words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_w2v = [AW2V_eachWord(model_w2v, review) for review in x_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de768b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517bd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9729732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
